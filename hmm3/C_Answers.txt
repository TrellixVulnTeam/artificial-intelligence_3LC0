Q7)
Data:
3 3 0.54 0.26 0.2 0.19 0.53 0.28 0.22 0.18 0.6
3 4 0.5 0.2 0.11 0.19 0.22 0.28 0.23 0.27 0.19 0.21 0.15 0.45
1 3 0.3 0.2 0.5

- Yes, both converge. Baum-Welch always does (but doesn't have to be
at a global maxima - could be in a local maxima as well although
this is not as likely if we do things right, such as not initializing
A and B exactly uniformly, but approximatively instead).
10k:  python3 hmm3.py < hmm_c_N10000.in
14850 (iterations)
3 3 0.6942811339673699 0.04489865023216916 0.26082021580046066 0.11766839530079967 0.7460771058286327 0.13625449887056879 0.15418587384984364 0.2566938178518232 0.5891203082983326
3 4 0.7099994962398879 0.18640862145684559 0.10359164843695491 2.3386631346917574e-07 0.09881201266130091 0.4211252559360722 0.3121739269713422 0.16788880443128756 0.03211319660009302 0.171325074506396 0.18662833974475376 0.6099333891487566

1k: python3 hmm3.py < hmm_c_N1000.in
3 3 0.6964764072237691 0.013355520467963426 0.2901680723082681 0.10146484237721358 0.8120129780896936 0.08652217953309357 0.19211580982560375 0.3012794969185915 0.5066046932558034
3 4 0.6887972824578699 0.22515641843057055 0.07537016018577528 0.0106761389257843 0.06786807834759359 0.41206675505247214 0.28139193370751686 0.23867323289241826 4.0752407194770376e-48 5.853711322911404e-13 0.35330159172209175 0.6466984082773223

- By having the 1k and the 10k observation sequences both converge as
described below, we can compare their final values to determine
if the 10k sequence better approximates lambda (which consists of A, B, pi).
The answer is that the 10k sequence does a much better job at
approximating lambda, although both 1k and 10k converge.

- You can define convergence as the point at which P(O|lambda) no longer
increases, i.e. the probability of the observation sequence given the
continually updated values of lambda does not increase anymore (in other
words, we have reached a global maxima - although this is never guaranteed).

Q8)
- How close to the parameters?
python3 hmm3.py < hmm_c_N10000.in
15209 (iterations)
3 3 0.6942811118431951 0.04489863837937411 0.2608202497774278 0.11766838787516655 0.7460770983543747 0.13625451377045394 0.15418589018352954 0.25669379823537913 0.5891203115810936
3 4 0.7099995167785185 0.18640862483505222 0.10359164826033344 2.1012609843879226e-07 0.09881201101245926 0.4211252613141535 0.3121739315136788 0.16788879615970564 0.032113199850804526 0.17132508223548484 0.18662834105230033 0.6099333768614107

In other words, fairly close, but could be better.
We just chose random values close to the actual values for the initialization.
These are the init matrices we chose:
3 3 0.65 0.06 0.29 0.13 0.75 0.12 0.15 0.26 0.59
3 4 0.75 0.07 0.179 0.01 0.07 0.37 0.32 0.24 0.01 0.12 0.22 0.659
1 3 0.99 0.005 0.005

- What is the problem with estimating distance & how to solve it?
The problem is knowing how to define the distance. There are many options
of comparing matrices. I would solve this problem by choosing a simple one
and using that as the main reference, such as d1 or d2 from here:
https://math.stackexchange.com/questions/507742/distance-similarity-between-two-matrices/508388

A problem with for instance d3 in the above link is that if only 2 values
in the matrices being compared are very far off but all other values are
perfect, d3 will tell us that the matrices are far apart. So we need
to choose something that reasonably takes all matrix values into account.

Q9)
Less than 3 states data:
2 2 0.45 0.55 0.52 0.48
2 4 0.22 0.27 0.19 0.32 0.26 0.18 0.23 0.33

Result:
python3 hmm3.py < hmm_c_N10000.in
2 2 0.8517439612965158 0.1482560387034907 0.340221589035065 0.6597784109649372
2 4 0.06154134164874713 0.3142140937892816 0.2597542184632555 0.3644903460987184 0.729334414716541 0.16819198393958534 0.09086327477635125 0.011610326567523295

- More/less than 3 states and why?
Less than 3 states and we are losing out on information. Say, if the
states are [hot, neutral, cold] then we may define hot as being above
20 degrees and cold below -20 degrees and neutral inbetween. In our resulting
calculations (after algorithm converges) we will get a lambda which includes
these possible states. But if we reduce the states to 2 we might be removing
the state "neutral", which essentially removes information, which means
the only classifications left are "hot" and "cold" and we lose out on
that specific -20 to 20 degree interval. We are thus generalizing too much
by having too few states.

Similarly, if we have say 100 states instead of 3, we will be too specific
in that each temperature classification (hot, neutral, cold) will be divided
into many small components which might not be necessary, and we are basically
adapting our calculations to the data set too much - also known as overfitting.

IF the above example is not sufficient, then another example is the one
with the vowels/consonants from the Brown Corpus. Just taking the
lower-case letters and whitespaces (27 characters) from the corpus,
and applying an HMM model to it (N=2 and M=27), i.e. where we
assume there are only 2 states, yields a resulting B matrix that tells us
which observations (characters) are vowels/consonants.

- 3 hidden states & 4 observations best choice? If not, why?
Yes, 3 hidden states and 4 observations are in this case the best choice
because the observation sequence was retrieved from a lambda with those
dimensions.

- How can you determine the optimal setting?
It depends on the size of the data and the information behind it, as outlined
in the answer to the first question (see above). But there are
algorithms that analyze the data set and try to estimate how many states
and observations are most probable.

More data will reasonably enough allow us to gather more statistically
significant information, i.e. we can have more hidden states.
An example of this is that the 2 researchers were able to obtain and
sensibly interpret the results for models with up to 12 hidden states
on the Brown Corpus, which has 1,000,000 English words.


Q10)
- Init with uniform distribution:
Uniform distribution - local maxima (see Stamp page 9)
Data: 3 3 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333
3 4 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333
1 3 0.33333333 0.33333333 0.33333333

Result:
python3 hmm3.py < hmm_c_N10000.in
2 (iterations)
3 3 0.3333333333332371 0.3333333333332371 0.3333333333332371 0.3333333333332371 0.3333333333332371 0.3333333333332371 0.3333333333332371 0.3333333333332371 0.3333333333332371
3 4 0.2641999999999775 0.2698999999999775 0.20849999999997784 0.25739999999997754 0.2641999999999775 0.2698999999999775 0.20849999999997784 0.25739999999997754 0.2641999999999775 0.2698999999999775 0.20849999999997784 0.25739999999997754

We instantly find a local maxima, so the algorithm converges within as
few as 2 iterations. The reason for this is because a uniform distribution
of probabilities makes all states & observations equally probable.
As a result, we are unable to move away from these parameters.

If we have a bunch of head observations and only one tail observation
from one coin (hidden state), obviously one coin is more likely to produce
head. BUT if we initialize a uniform distribution of values,
it doesn't matter which coin we choose because we'll always get the
same distribution for the observation sequence (h,h,h,h,h,h,t).
As a result we won't be able to move away from it because the uniform
initialization explains the observations so well.

- Init with diagonal matrix A and pi=[0 0 1]
Data:
3 3 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0
3 4 0.75 0.07 0.179 0.01 0.07 0.37 0.32 0.24 0.01 0.12 0.22 0.659
1 3 0.0 0.0 1.0

Result (for 10k observations):
1
2
3 3 nan nan nan nan nan nan nan nan nan
3 4 nan nan nan nan nan nan nan nan nan nan nan nan

We're working with a lot of zeros, so it becomes problematic, as follows:

The result is NaN-values which are the result of undefined
mathematical operations. For 10k observations, denom is already NaN
(beta-values become NaN first) and so we get division by NaN, which is NaN,
ultimately resulting in the output given above.
So the NaN-values in Beta infect all other values, basically.

For 1k observations, when
re-estimating alpha and beta (numer/denom), denom will be zero,
which means we get an exception and the program terminates.

Note that denom = sum(self.gamma[t][i]) (line 102 in our code)

---------------

For 10k observations, the values in beta end up becoming infinity,
and at some point we multiply 0 * inf = NaN, which infects all other
values, resulting in what we described above (division by NaN,
which doesn't throw an exception but makes all values NaN).

For 1k observations, the values in beta don't end up becoming infinity
(because Python can represent values that large), so instead we
get division by zero.

---------------

- Init with values close to the solution:
Question is how close? Oh well. Let's try something:
Data:
3 3 0.699 0.041 0.26 0.099 0.791 0.11 0.199 0.291 0.51
3 4 0.699 0.201 0.099 0.001 0.099 0.391 0.305 0.205 0.001 0.099 0.199 0.701
1 3 0.999 0.0005 0.0005

Result:
3 3 0.694281209793805 0.0448986908585881 0.260820099347603 0.11766842075099504 0.7460771314485708 0.13625444780043244 0.15418581786936808 0.25669388508729246 0.5891202970433391
3 4 0.7099994258478713 0.18640860987842814 0.10359164904214808 3.1523154836674483e-07 0.0988120183122425 0.42112523750163366 0.3121739114022405 0.16788883278388414 0.03211318545782662 0.1713250480137407 0.18662833526207143 0.6099334312663607

There is always going to be a margin of error in estimating the true
values based on the observation sequence (note that we are estimating,
after all!), which means that even though our initial values
are quite close to the true values, we still won't converge to these
true values, instead we will converge to the estimated values (i.e.
the values with the errors).
